{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a3593a-7704-4652-b083-e072b05e55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b11881-04be-42aa-b237-736c4ba7d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завантаження даних \n",
    "data = pd.read_csv(r'cheese_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f11b53e-2941-47c8-992e-8b2eb0db9ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CheeseId', 'ManufacturerProvCode', 'ManufacturingTypeEn',\n",
       "       'MoisturePercent', 'FlavourEn', 'CharacteristicsEn', 'Organic',\n",
       "       'CategoryTypeEn', 'MilkTypeEn', 'MilkTreatmentTypeEn', 'RindTypeEn',\n",
       "       'CheeseName', 'FatLevel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Виведення колонок\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7945def8-68cd-4d0c-9fa6-43c753b1f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksy\\AppData\\Local\\Temp\\ipykernel_10960\\3279476893.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].median(), inplace=True)\n",
      "C:\\Users\\maksy\\AppData\\Local\\Temp\\ipykernel_10960\\3279476893.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].median(), inplace=True)\n",
      "C:\\Users\\maksy\\AppData\\Local\\Temp\\ipykernel_10960\\3279476893.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Заповнення числових NaN медіаною\n",
    "for column in data.select_dtypes(include=[np.number]).columns:\n",
    "    data[column].fillna(data[column].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8fc006-d323-4d6a-ba6a-a195bb172d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Встановлюємо CategoryTypeEn як таргет\n",
    "target = 'CategoryTypeEn'\n",
    "data[target] = LabelEncoder().fit_transform(data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e01fbf3-8442-4dcb-a533-cb8042d43583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підготовка даних\n",
    "data_encoded = pd.get_dummies(data.drop(columns=[target, 'CheeseId', 'CheeseName']), drop_first=True)\n",
    "X = data_encoded\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133d9640-d212-4162-bb98-615918299181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормалізація\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b14824-e214-4460-b987-b458c41dae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Розділення на тренувальні та тестові дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24050b24-0156-4b58-8951-4cb99619ca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53079542,  3.08630078, -0.25793448, ...,  1.2566654 ,\n",
       "       -0.40687602,  0.72345823])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe72a33-aeae-4c17-af1d-403710a80b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 43.4324, Accuracy: 22.63%\n",
      "Epoch 2/20, Loss: 35.1455, Accuracy: 38.96%\n",
      "Epoch 3/20, Loss: 29.4354, Accuracy: 58.30%\n",
      "Epoch 4/20, Loss: 22.1171, Accuracy: 72.02%\n",
      "Epoch 5/20, Loss: 15.2687, Accuracy: 82.17%\n",
      "Epoch 6/20, Loss: 11.1140, Accuracy: 85.46%\n",
      "Epoch 7/20, Loss: 9.2344, Accuracy: 86.83%\n",
      "Epoch 8/20, Loss: 7.9497, Accuracy: 89.16%\n",
      "Epoch 9/20, Loss: 6.9756, Accuracy: 90.67%\n",
      "Epoch 10/20, Loss: 6.7650, Accuracy: 90.81%\n",
      "Epoch 11/20, Loss: 6.3836, Accuracy: 91.08%\n",
      "Epoch 12/20, Loss: 6.0302, Accuracy: 91.77%\n",
      "Epoch 13/20, Loss: 5.3447, Accuracy: 92.59%\n",
      "Epoch 14/20, Loss: 5.3849, Accuracy: 91.91%\n",
      "Epoch 15/20, Loss: 4.8863, Accuracy: 93.00%\n",
      "Epoch 16/20, Loss: 4.8556, Accuracy: 93.14%\n",
      "Epoch 17/20, Loss: 4.8212, Accuracy: 93.14%\n",
      "Epoch 18/20, Loss: 4.7545, Accuracy: 92.87%\n",
      "Epoch 19/20, Loss: 4.9367, Accuracy: 93.14%\n",
      "Epoch 20/20, Loss: 4.2364, Accuracy: 94.10%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60       101\n",
      "           1       0.34      0.58      0.43        31\n",
      "           2       0.50      0.10      0.17        10\n",
      "           3       0.37      0.43      0.40        67\n",
      "           4       0.62      0.51      0.56        89\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.50       313\n",
      "   macro avg       0.35      0.32      0.31       313\n",
      "weighted avg       0.50      0.50      0.49       313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksy\\anaconda3\\envs\\test_rag\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\maksy\\anaconda3\\envs\\test_rag\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\maksy\\anaconda3\\envs\\test_rag\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Перетворення даних у тензори\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Створення DataLoader для тренувальних і тестових даних\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Модель повнозв'язаної нейронної мережі\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Ініціалізація моделі, оптимізатора і функції втрат\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(set(y_train))\n",
    "model = FullyConnectedNN(input_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Тренування моделі\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Оцінка моделі\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "        y_true.extend(y_batch.numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd47d5c-9f62-4f02-8b90-90fca5cab2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Classes: ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
      "Epoch 1/30, Loss: 29.8972, Accuracy: 11.12%\n",
      "Epoch 2/30, Loss: 29.1299, Accuracy: 15.88%\n",
      "Epoch 3/30, Loss: 27.9596, Accuracy: 19.38%\n",
      "Epoch 4/30, Loss: 26.2478, Accuracy: 24.38%\n",
      "Epoch 5/30, Loss: 24.7641, Accuracy: 28.88%\n",
      "Epoch 6/30, Loss: 23.4825, Accuracy: 32.75%\n",
      "Epoch 7/30, Loss: 22.3756, Accuracy: 36.25%\n",
      "Epoch 8/30, Loss: 20.8989, Accuracy: 39.38%\n",
      "Epoch 9/30, Loss: 20.8231, Accuracy: 39.75%\n",
      "Epoch 10/30, Loss: 19.3130, Accuracy: 45.00%\n",
      "Epoch 11/30, Loss: 18.5182, Accuracy: 48.88%\n",
      "Epoch 12/30, Loss: 17.5198, Accuracy: 51.25%\n",
      "Epoch 13/30, Loss: 15.7409, Accuracy: 57.25%\n",
      "Epoch 14/30, Loss: 14.1029, Accuracy: 59.38%\n",
      "Epoch 15/30, Loss: 13.6625, Accuracy: 63.12%\n",
      "Epoch 16/30, Loss: 11.7604, Accuracy: 67.38%\n",
      "Epoch 17/30, Loss: 11.3432, Accuracy: 69.38%\n",
      "Epoch 18/30, Loss: 10.6285, Accuracy: 70.00%\n",
      "Epoch 19/30, Loss: 8.4119, Accuracy: 77.25%\n",
      "Epoch 20/30, Loss: 7.4623, Accuracy: 79.50%\n",
      "Epoch 21/30, Loss: 6.6672, Accuracy: 81.88%\n",
      "Epoch 22/30, Loss: 6.0112, Accuracy: 84.38%\n",
      "Epoch 23/30, Loss: 5.7884, Accuracy: 84.88%\n",
      "Epoch 24/30, Loss: 5.8954, Accuracy: 83.62%\n",
      "Epoch 25/30, Loss: 5.2568, Accuracy: 86.38%\n",
      "Epoch 26/30, Loss: 4.6557, Accuracy: 87.50%\n",
      "Epoch 27/30, Loss: 3.4648, Accuracy: 91.12%\n",
      "Epoch 28/30, Loss: 3.5259, Accuracy: 90.50%\n",
      "Epoch 29/30, Loss: 3.1722, Accuracy: 92.75%\n",
      "Epoch 30/30, Loss: 3.0680, Accuracy: 92.38%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.79      0.94      0.86        16\n",
      "        bird       0.33      0.16      0.21        19\n",
      "         car       0.57      0.57      0.57        23\n",
      "         cat       0.30      0.15      0.20        20\n",
      "        deer       0.60      0.44      0.51        34\n",
      "         dog       0.37      0.35      0.36        20\n",
      "       horse       0.38      0.69      0.49        13\n",
      "      monkey       0.28      0.47      0.35        17\n",
      "        ship       0.67      0.60      0.63        20\n",
      "       truck       0.29      0.39      0.33        18\n",
      "\n",
      "    accuracy                           0.46       200\n",
      "   macro avg       0.46      0.48      0.45       200\n",
      "weighted avg       0.47      0.46      0.45       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Гіперпараметри\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# Завантаження та трансформація датасету STL10\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Завантаження повного датасету\n",
    "full_train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
    "full_test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "# Вибір підмножини з 1000 об'єктів\n",
    "np.random.seed(42)  # Для відтворюваності\n",
    "train_indices = np.random.choice(len(full_train_dataset), 800, replace=False)  # 800 об'єктів для тренування\n",
    "test_indices = np.random.choice(len(full_test_dataset), 200, replace=False)  # 200 об'єктів для тестування\n",
    "\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "test_dataset = Subset(full_test_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Огляд класів\n",
    "classes = full_train_dataset.classes\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# Згорткова нейронна мережа\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # RGB (3 канали)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 12 * 12, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)  # 10 класів\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Ініціалізація моделі, оптимізатора і функції втрат\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Тренування\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Оцінка\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450597b4-a574-455f-9e85-076cf0793b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maksy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.6286876201629639\n",
      "Epoch 2/5, Loss: 1.5749417543411255\n",
      "Epoch 3/5, Loss: 1.5224155187606812\n",
      "Epoch 4/5, Loss: 1.4710476398468018\n",
      "Epoch 5/5, Loss: 1.4207502603530884\n",
      "Test Accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Завантаження стоп-слів\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Завантаження даних\n",
    "data = pd.read_csv('twitter_training.csv')\n",
    "data.columns = ['ID', 'Category', 'Sentiment', 'Tweet']\n",
    "\n",
    "# Перевірка на пропущені значення та заповнення їх порожнім рядком\n",
    "data['Tweet'] = data['Tweet'].fillna('')\n",
    "\n",
    "# Очищення тексту\n",
    "data['cleaned_text'] = data['Tweet'].str.lower().str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "data['cleaned_text'] = data['cleaned_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Енкодинг меток\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Розділення на тренувальні та тестові дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['Sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Токенізація та побудова словника\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(X_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Токенізація та перетворення текстів у числові представлення\n",
    "def text_to_sequence(texts):\n",
    "    return [vocab(tokenizer(text)) for text in texts]\n",
    "\n",
    "X_train_seq = text_to_sequence(X_train)\n",
    "X_test_seq = text_to_sequence(X_test)\n",
    "\n",
    "# Паддінг послідовностей\n",
    "def pad_sequences(sequences, max_len):\n",
    "    return [seq[:max_len] + [0] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, max_len)\n",
    "\n",
    "# Перетворення на PyTorch тензори\n",
    "X_train_pad = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_pad = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Створення PyTorch Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train_pad, y_train)\n",
    "test_dataset = TextDataset(X_test_pad, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Визначення моделі\n",
    "class RNNModelFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModelFromScratch, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Ембедінги для слів\n",
    "        output, hidden = self.rnn(embedded)  # RNN обчислення\n",
    "        last_hidden = hidden[-1]  # Беремо останній стан прихованого шару\n",
    "        return self.fc(last_hidden)  # Логіти\n",
    "\n",
    "# Ініціалізація параметрів\n",
    "vocab_size = 10000  # Приклад: розмір словника\n",
    "embedding_dim = 128  # Розмірність ембедінгів\n",
    "hidden_dim = 64  # Розмір прихованого шару\n",
    "output_dim = 5  # Приклад: кількість класів (для класифікації)\n",
    "num_epochs = 5  # Кількість епох\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Створення моделі\n",
    "model = RNNModelFromScratch(vocab_size, embedding_dim, hidden_dim, output_dim).to('cpu')\n",
    "\n",
    "# Функція втрат і оптимізатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Припускаємо, що X_train і y_train вже підготовлені\n",
    "# X_train має бути розмірності (batch_size, sequence_length)\n",
    "# y_train має бути розмірності (batch_size)\n",
    "\n",
    "# Генерація фейкових даних для прикладу\n",
    "X_train = torch.randint(0, vocab_size, (100, 50)).to('cpu')  # 100 прикладів, послідовності довжиною 50\n",
    "y_train = torch.randint(0, output_dim, (100,)).to('cpu')  # 100 міток\n",
    "\n",
    "# Навчання\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Увімкнення режиму навчання\n",
    "    optimizer.zero_grad()  # Скидання градієнтів\n",
    "\n",
    "    # Передбачення\n",
    "    predictions = model(X_train)\n",
    "    loss = criterion(predictions, y_train)  # Розрахунок втрат\n",
    "\n",
    "    loss.backward()  # Зворотне розповсюдження\n",
    "    optimizer.step()  # Оновлення ваг\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Валідація\n",
    "model.eval()  # Увімкнення режиму оцінки\n",
    "with torch.no_grad():\n",
    "    # Генерація фейкових тестових даних\n",
    "    X_test = torch.randint(0, vocab_size, (20, 50)).to('cpu')  # 20 тестових прикладів\n",
    "    y_test = torch.randint(0, output_dim, (20,)).to('cpu')  # 20 міток\n",
    "\n",
    "    predictions = model(X_test)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)  # Отримання класів\n",
    "    accuracy = (predicted_labels == y_test).float().mean()  # Розрахунок точності\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db53416-d8f2-4696-8af2-e3995ee2421e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m glove_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove.6B.50d.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update with the correct path to the GloVe file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m---> 23\u001b[0m glove_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Vocabulary (example vocabulary, replace with your actual vocab)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m}  \u001b[38;5;66;03m# Example vocab\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mload_glove_embeddings\u001b[1;34m(glove_file, embedding_dim)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_glove_embeddings\u001b[39m(glove_file, embedding_dim):\n\u001b[0;32m     10\u001b[0m     glove_vectors \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglove_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m             values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test_rag\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to load GloVe embeddings from a file\n",
    "def load_glove_embeddings(glove_file, embedding_dim):\n",
    "    glove_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_vectors[word] = vector\n",
    "    return glove_vectors\n",
    "\n",
    "# Load GloVe pre-trained embeddings (50-dimensional as an example)\n",
    "glove_file = \"glove.6B.50d.txt\"  # Update with the correct path to the GloVe file\n",
    "embedding_dim = 50\n",
    "\n",
    "glove_vectors = load_glove_embeddings(glove_file, embedding_dim)\n",
    "\n",
    "# Vocabulary (example vocabulary, replace with your actual vocab)\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"hello\": 2, \"world\": 3}  # Example vocab\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Build embedding matrix using pre-trained GloVe vectors\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[idx] = glove_vectors[word]\n",
    "    else:\n",
    "        # Random initialization for words not in the GloVe vocabulary\n",
    "        embedding_matrix[idx] = np.random.normal(0, 1, embedding_dim)\n",
    "\n",
    "# Define the RNN model with pre-trained embeddings\n",
    "class RNNModelPretrained(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, embedding_matrix):\n",
    "        super(RNNModelPretrained, self).__init__()\n",
    "        # Pre-trained embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h_0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last RNN cell\n",
    "        return out\n",
    "\n",
    "# Initialize the model using pre-trained embeddings\n",
    "hidden_dim = 128\n",
    "output_size = 3  # Example number of classes\n",
    "model_pretrained = RNNModelPretrained(vocab_size, embedding_dim, hidden_dim, output_size, embedding_matrix)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_pretrained = model_pretrained.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_pretrained.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 10\n",
    "# Assume train_loader is a DataLoader with your dataset\n",
    "for epoch in range(num_epochs):\n",
    "    model_pretrained.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_pretrained(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Example evaluation loop\n",
    "model_pretrained.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model_pretrained(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Classification report for detailed evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model_pretrained(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b7e337-997e-4bba-8f77-86ff1e87988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Classes: ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
      "Epoch 1/30, Loss: 2.3052, Accuracy: 4.69%\n",
      "Epoch 1/30, Loss: 6.2823, Accuracy: 6.25%\n",
      "Epoch 1/30, Loss: 8.6924, Accuracy: 7.29%\n",
      "Epoch 1/30, Loss: 10.9913, Accuracy: 9.77%\n",
      "Epoch 1/30, Loss: 13.2957, Accuracy: 9.69%\n",
      "Epoch 1/30, Loss: 15.6014, Accuracy: 9.64%\n",
      "Epoch 1/30, Loss: 17.8979, Accuracy: 10.27%\n",
      "Epoch 1/30, Loss: 20.2322, Accuracy: 10.35%\n",
      "Epoch 1/30, Loss: 22.5344, Accuracy: 9.90%\n",
      "Epoch 1/30, Loss: 24.8415, Accuracy: 9.69%\n",
      "Epoch 1/30, Loss: 27.1575, Accuracy: 9.66%\n",
      "Epoch 1/30, Loss: 29.4621, Accuracy: 10.42%\n",
      "Epoch 1/30, Loss: 31.7808, Accuracy: 10.12%\n",
      "Epoch 2/30, Loss: 2.2996, Accuracy: 6.25%\n",
      "Epoch 2/30, Loss: 4.6073, Accuracy: 6.25%\n",
      "Epoch 2/30, Loss: 6.8898, Accuracy: 8.33%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     87\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m---> 88\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     91\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test_rag\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test_rag\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test_rag\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Гіперпараметри\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# Завантаження та трансформація датасету STL10\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Завантаження повного датасету\n",
    "full_train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
    "full_test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "# Вибір підмножини з 1000 об'єктів\n",
    "np.random.seed(42)  # Для відтворюваності\n",
    "train_indices = np.random.choice(len(full_train_dataset), 800, replace=False)  # 800 об'єктів для тренування\n",
    "test_indices = np.random.choice(len(full_test_dataset), 200, replace=False)  # 200 об'єктів для тестування\n",
    "\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "test_dataset = Subset(full_test_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Огляд класів\n",
    "classes = full_train_dataset.classes\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# Модифікація VGG-14\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        # Завантаження попередньо навченого VGG-16\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        \n",
    "        # Замінюємо перший шар для роботи із зображеннями розміру 96x96\n",
    "        vgg16.features[0] = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Зберігаємо лише необхідні шари\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        # Замінюємо класифікатор\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 3 * 3, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Ініціалізація моделі, оптимізатора і функції втрат\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ModifiedVGG(num_classes=len(classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Тренування\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Оцінка\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2a18b-f4ab-49e6-8cae-ce3dfff85bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d45d53-eb42-4975-9818-ea10ccc20660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2b50f-4d59-4c0d-8d9f-c77e04917bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
