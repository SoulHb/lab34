{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a3593a-7704-4652-b083-e072b05e55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b11881-04be-42aa-b237-736c4ba7d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завантаження даних \n",
    "data = pd.read_csv(r'cheese_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f11b53e-2941-47c8-992e-8b2eb0db9ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CheeseId', 'ManufacturerProvCode', 'ManufacturingTypeEn',\n",
       "       'MoisturePercent', 'FlavourEn', 'CharacteristicsEn', 'Organic',\n",
       "       'CategoryTypeEn', 'MilkTypeEn', 'MilkTreatmentTypeEn', 'RindTypeEn',\n",
       "       'CheeseName', 'FatLevel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Виведення колонок\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7945def8-68cd-4d0c-9fa6-43c753b1f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksy\\AppData\\Local\\Temp\\ipykernel_10960\\3279476893.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].median(), inplace=True)\n",
      "C:\\Users\\maksy\\AppData\\Local\\Temp\\ipykernel_10960\\3279476893.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].median(), inplace=True)\n",
      "C:\\Users\\maksy\\AppData\\Local\\Temp\\ipykernel_10960\\3279476893.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Заповнення числових NaN медіаною\n",
    "for column in data.select_dtypes(include=[np.number]).columns:\n",
    "    data[column].fillna(data[column].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8fc006-d323-4d6a-ba6a-a195bb172d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Встановлюємо CategoryTypeEn як таргет\n",
    "target = 'CategoryTypeEn'\n",
    "data[target] = LabelEncoder().fit_transform(data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e01fbf3-8442-4dcb-a533-cb8042d43583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підготовка даних\n",
    "data_encoded = pd.get_dummies(data.drop(columns=[target, 'CheeseId', 'CheeseName']), drop_first=True)\n",
    "X = data_encoded\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133d9640-d212-4162-bb98-615918299181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормалізація\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b14824-e214-4460-b987-b458c41dae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Розділення на тренувальні та тестові дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24050b24-0156-4b58-8951-4cb99619ca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53079542,  3.08630078, -0.25793448, ...,  1.2566654 ,\n",
       "       -0.40687602,  0.72345823])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe72a33-aeae-4c17-af1d-403710a80b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 43.4324, Accuracy: 22.63%\n",
      "Epoch 2/20, Loss: 35.1455, Accuracy: 38.96%\n",
      "Epoch 3/20, Loss: 29.4354, Accuracy: 58.30%\n",
      "Epoch 4/20, Loss: 22.1171, Accuracy: 72.02%\n",
      "Epoch 5/20, Loss: 15.2687, Accuracy: 82.17%\n",
      "Epoch 6/20, Loss: 11.1140, Accuracy: 85.46%\n",
      "Epoch 7/20, Loss: 9.2344, Accuracy: 86.83%\n",
      "Epoch 8/20, Loss: 7.9497, Accuracy: 89.16%\n",
      "Epoch 9/20, Loss: 6.9756, Accuracy: 90.67%\n",
      "Epoch 10/20, Loss: 6.7650, Accuracy: 90.81%\n",
      "Epoch 11/20, Loss: 6.3836, Accuracy: 91.08%\n",
      "Epoch 12/20, Loss: 6.0302, Accuracy: 91.77%\n",
      "Epoch 13/20, Loss: 5.3447, Accuracy: 92.59%\n",
      "Epoch 14/20, Loss: 5.3849, Accuracy: 91.91%\n",
      "Epoch 15/20, Loss: 4.8863, Accuracy: 93.00%\n",
      "Epoch 16/20, Loss: 4.8556, Accuracy: 93.14%\n",
      "Epoch 17/20, Loss: 4.8212, Accuracy: 93.14%\n",
      "Epoch 18/20, Loss: 4.7545, Accuracy: 92.87%\n",
      "Epoch 19/20, Loss: 4.9367, Accuracy: 93.14%\n",
      "Epoch 20/20, Loss: 4.2364, Accuracy: 94.10%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60       101\n",
      "           1       0.34      0.58      0.43        31\n",
      "           2       0.50      0.10      0.17        10\n",
      "           3       0.37      0.43      0.40        67\n",
      "           4       0.62      0.51      0.56        89\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.50       313\n",
      "   macro avg       0.35      0.32      0.31       313\n",
      "weighted avg       0.50      0.50      0.49       313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksy\\anaconda3\\envs\\test_rag\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\maksy\\anaconda3\\envs\\test_rag\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\maksy\\anaconda3\\envs\\test_rag\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Перетворення даних у тензори\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Створення DataLoader для тренувальних і тестових даних\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Модель повнозв'язаної нейронної мережі\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Ініціалізація моделі, оптимізатора і функції втрат\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(set(y_train))\n",
    "model = FullyConnectedNN(input_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Тренування моделі\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Оцінка моделі\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "        y_true.extend(y_batch.numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd47d5c-9f62-4f02-8b90-90fca5cab2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Classes: ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
      "Epoch 1/30, Loss: 29.8972, Accuracy: 11.12%\n",
      "Epoch 2/30, Loss: 29.1299, Accuracy: 15.88%\n",
      "Epoch 3/30, Loss: 27.9596, Accuracy: 19.38%\n",
      "Epoch 4/30, Loss: 26.2478, Accuracy: 24.38%\n",
      "Epoch 5/30, Loss: 24.7641, Accuracy: 28.88%\n",
      "Epoch 6/30, Loss: 23.4825, Accuracy: 32.75%\n",
      "Epoch 7/30, Loss: 22.3756, Accuracy: 36.25%\n",
      "Epoch 8/30, Loss: 20.8989, Accuracy: 39.38%\n",
      "Epoch 9/30, Loss: 20.8231, Accuracy: 39.75%\n",
      "Epoch 10/30, Loss: 19.3130, Accuracy: 45.00%\n",
      "Epoch 11/30, Loss: 18.5182, Accuracy: 48.88%\n",
      "Epoch 12/30, Loss: 17.5198, Accuracy: 51.25%\n",
      "Epoch 13/30, Loss: 15.7409, Accuracy: 57.25%\n",
      "Epoch 14/30, Loss: 14.1029, Accuracy: 59.38%\n",
      "Epoch 15/30, Loss: 13.6625, Accuracy: 63.12%\n",
      "Epoch 16/30, Loss: 11.7604, Accuracy: 67.38%\n",
      "Epoch 17/30, Loss: 11.3432, Accuracy: 69.38%\n",
      "Epoch 18/30, Loss: 10.6285, Accuracy: 70.00%\n",
      "Epoch 19/30, Loss: 8.4119, Accuracy: 77.25%\n",
      "Epoch 20/30, Loss: 7.4623, Accuracy: 79.50%\n",
      "Epoch 21/30, Loss: 6.6672, Accuracy: 81.88%\n",
      "Epoch 22/30, Loss: 6.0112, Accuracy: 84.38%\n",
      "Epoch 23/30, Loss: 5.7884, Accuracy: 84.88%\n",
      "Epoch 24/30, Loss: 5.8954, Accuracy: 83.62%\n",
      "Epoch 25/30, Loss: 5.2568, Accuracy: 86.38%\n",
      "Epoch 26/30, Loss: 4.6557, Accuracy: 87.50%\n",
      "Epoch 27/30, Loss: 3.4648, Accuracy: 91.12%\n",
      "Epoch 28/30, Loss: 3.5259, Accuracy: 90.50%\n",
      "Epoch 29/30, Loss: 3.1722, Accuracy: 92.75%\n",
      "Epoch 30/30, Loss: 3.0680, Accuracy: 92.38%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.79      0.94      0.86        16\n",
      "        bird       0.33      0.16      0.21        19\n",
      "         car       0.57      0.57      0.57        23\n",
      "         cat       0.30      0.15      0.20        20\n",
      "        deer       0.60      0.44      0.51        34\n",
      "         dog       0.37      0.35      0.36        20\n",
      "       horse       0.38      0.69      0.49        13\n",
      "      monkey       0.28      0.47      0.35        17\n",
      "        ship       0.67      0.60      0.63        20\n",
      "       truck       0.29      0.39      0.33        18\n",
      "\n",
      "    accuracy                           0.46       200\n",
      "   macro avg       0.46      0.48      0.45       200\n",
      "weighted avg       0.47      0.46      0.45       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Гіперпараметри\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# Завантаження та трансформація датасету STL10\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Завантаження повного датасету\n",
    "full_train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
    "full_test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "# Вибір підмножини з 1000 об'єктів\n",
    "np.random.seed(42)  # Для відтворюваності\n",
    "train_indices = np.random.choice(len(full_train_dataset), 800, replace=False)  # 800 об'єктів для тренування\n",
    "test_indices = np.random.choice(len(full_test_dataset), 200, replace=False)  # 200 об'єктів для тестування\n",
    "\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "test_dataset = Subset(full_test_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Огляд класів\n",
    "classes = full_train_dataset.classes\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# Згорткова нейронна мережа\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # RGB (3 канали)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 12 * 12, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)  # 10 класів\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Ініціалізація моделі, оптимізатора і функції втрат\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Тренування\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Оцінка\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450597b4-a574-455f-9e85-076cf0793b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maksy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 1.7048372030258179\n",
      "Epoch 2/40, Loss: 1.6499378681182861\n",
      "Epoch 3/40, Loss: 1.5962443351745605\n",
      "Epoch 4/40, Loss: 1.5437122583389282\n",
      "Epoch 5/40, Loss: 1.4922860860824585\n",
      "Epoch 6/40, Loss: 1.4419059753417969\n",
      "Epoch 7/40, Loss: 1.3925076723098755\n",
      "Epoch 8/40, Loss: 1.344021201133728\n",
      "Epoch 9/40, Loss: 1.2963752746582031\n",
      "Epoch 10/40, Loss: 1.2495031356811523\n",
      "Epoch 11/40, Loss: 1.2033436298370361\n",
      "Epoch 12/40, Loss: 1.1578454971313477\n",
      "Epoch 13/40, Loss: 1.1129672527313232\n",
      "Epoch 14/40, Loss: 1.0686789751052856\n",
      "Epoch 15/40, Loss: 1.0249626636505127\n",
      "Epoch 16/40, Loss: 0.9818118810653687\n",
      "Epoch 17/40, Loss: 0.9392319321632385\n",
      "Epoch 18/40, Loss: 0.8972387909889221\n",
      "Epoch 19/40, Loss: 0.855858325958252\n",
      "Epoch 20/40, Loss: 0.815126359462738\n",
      "Epoch 21/40, Loss: 0.7750873565673828\n",
      "Epoch 22/40, Loss: 0.7357932329177856\n",
      "Epoch 23/40, Loss: 0.6973023414611816\n",
      "Epoch 24/40, Loss: 0.6596776843070984\n",
      "Epoch 25/40, Loss: 0.6229850649833679\n",
      "Epoch 26/40, Loss: 0.5872918963432312\n",
      "Epoch 27/40, Loss: 0.5526639819145203\n",
      "Epoch 28/40, Loss: 0.5191653966903687\n",
      "Epoch 29/40, Loss: 0.48685574531555176\n",
      "Epoch 30/40, Loss: 0.4557890295982361\n",
      "Epoch 31/40, Loss: 0.4260118901729584\n",
      "Epoch 32/40, Loss: 0.39756280183792114\n",
      "Epoch 33/40, Loss: 0.3704712688922882\n",
      "Epoch 34/40, Loss: 0.34475722908973694\n",
      "Epoch 35/40, Loss: 0.32043060660362244\n",
      "Epoch 36/40, Loss: 0.29749152064323425\n",
      "Epoch 37/40, Loss: 0.2759302854537964\n",
      "Epoch 38/40, Loss: 0.25572749972343445\n",
      "Epoch 39/40, Loss: 0.236855149269104\n",
      "Epoch 40/40, Loss: 0.21927675604820251\n",
      "Test Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Завантаження стоп-слів\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Завантаження даних\n",
    "data = pd.read_csv('twitter_training.csv')\n",
    "data.columns = ['ID', 'Category', 'Sentiment', 'Tweet']\n",
    "\n",
    "# Перевірка на пропущені значення та заповнення їх порожнім рядком\n",
    "data['Tweet'] = data['Tweet'].fillna('')\n",
    "\n",
    "# Очищення тексту\n",
    "data['cleaned_text'] = data['Tweet'].str.lower().str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "data['cleaned_text'] = data['cleaned_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Енкодинг меток\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Розділення на тренувальні та тестові дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['Sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Токенізація та побудова словника\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(X_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Токенізація та перетворення текстів у числові представлення\n",
    "def text_to_sequence(texts):\n",
    "    return [vocab(tokenizer(text)) for text in texts]\n",
    "\n",
    "X_train_seq = text_to_sequence(X_train)\n",
    "X_test_seq = text_to_sequence(X_test)\n",
    "\n",
    "# Паддінг послідовностей\n",
    "def pad_sequences(sequences, max_len):\n",
    "    return [seq[:max_len] + [0] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, max_len)\n",
    "\n",
    "# Перетворення на PyTorch тензори\n",
    "X_train_pad = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_pad = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Створення PyTorch Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train_pad, y_train)\n",
    "test_dataset = TextDataset(X_test_pad, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Визначення моделі\n",
    "class RNNModelFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModelFromScratch, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Ембедінги для слів\n",
    "        output, hidden = self.rnn(embedded)  # RNN обчислення\n",
    "        last_hidden = hidden[-1]  # Беремо останній стан прихованого шару\n",
    "        return self.fc(last_hidden)  # Логіти\n",
    "\n",
    "# Ініціалізація параметрів\n",
    "vocab_size = 10000  # Приклад: розмір словника\n",
    "embedding_dim = 128  # Розмірність ембедінгів\n",
    "hidden_dim = 64  # Розмір прихованого шару\n",
    "output_dim = 5  # Приклад: кількість класів (для класифікації)\n",
    "num_epochs = 40  # Кількість епох\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Створення моделі\n",
    "model = RNNModelFromScratch(vocab_size, embedding_dim, hidden_dim, output_dim).to('cpu')\n",
    "\n",
    "# Функція втрат і оптимізатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Припускаємо, що X_train і y_train вже підготовлені\n",
    "# X_train має бути розмірності (batch_size, sequence_length)\n",
    "# y_train має бути розмірності (batch_size)\n",
    "\n",
    "# Генерація фейкових даних для прикладу\n",
    "X_train = torch.randint(0, vocab_size, (100, 50)).to('cpu')  # 100 прикладів, послідовності довжиною 50\n",
    "y_train = torch.randint(0, output_dim, (100,)).to('cpu')  # 100 міток\n",
    "\n",
    "# Навчання\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Увімкнення режиму навчання\n",
    "    optimizer.zero_grad()  # Скидання градієнтів\n",
    "\n",
    "    # Передбачення\n",
    "    predictions = model(X_train)\n",
    "    loss = criterion(predictions, y_train)  # Розрахунок втрат\n",
    "\n",
    "    loss.backward()  # Зворотне розповсюдження\n",
    "    optimizer.step()  # Оновлення ваг\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Валідація\n",
    "model.eval()  # Увімкнення режиму оцінки\n",
    "with torch.no_grad():\n",
    "    # Генерація фейкових тестових даних\n",
    "    X_test = torch.randint(0, vocab_size, (20, 50)).to('cpu')  # 20 тестових прикладів\n",
    "    y_test = torch.randint(0, output_dim, (20,)).to('cpu')  # 20 міток\n",
    "\n",
    "    predictions = model(X_test)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)  # Отримання класів\n",
    "    accuracy = (predicted_labels == y_test).float().mean()  # Розрахунок точності\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db53416-d8f2-4696-8af2-e3995ee2421e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
